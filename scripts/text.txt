Komputer  dawniej: mózg elektronowy, elektroniczna maszyna cyfrowa, maszyna matematyczna – maszyna przeznaczona do przetwarzania informacji, które da się zapisać w formie ciągu cyfr albo sygnału ciągłego. Maszyna roku tygodnika „Time” w 1982 roku.

Mimo że mechaniczne maszyny liczące istniały od wielu stuleci, komputery w sensie współczesnym pojawiły się dopiero w połowie XX wieku, gdy zbudowano pierwsze komputery elektroniczne. Miały one rozmiary sporych pomieszczeń i zużywały kilkaset razy więcej energii niż współczesne komputery osobiste (PC), a jednocześnie miały miliardy razy mniejszą moc obliczeniową. Współcześnie są prowadzone także badania nad komputerami biologicznymi i optycznymi.

Małe komputery mogą zmieścić się nawet w zegarku i są zasilane baterią. Komputery osobiste stały się symbolem ery informatycznej i większość utożsamia je z „komputerem” właśnie. Najliczniejszymi maszynami liczącymi są systemy wbudowane sterujące najróżniejszymi urządzeniami – od odtwarzaczy MP3 i zabawek po roboty przemysłowe. 

Komputer od typowego kalkulatora odróżnia zdolność wykonywania wielokrotnie, automatycznie powtarzanych obliczeń, według algorytmicznego wzorca zwanego programem, gdy tymczasem kalkulator może zwykle wykonywać tylko pojedyncze działania. Granica jest tu umowna, ponieważ taką definicję komputera spełniają też kalkulatory programowalne (naukowe, inżynierskie), jednak kalkulatory służą tylko do obliczeń numerycznych, podczas gdy nazwa komputer najczęściej dotyczy urządzeń wielofunkcyjnych.
Global Digital Divide1.png

Jakkolwiek istnieją mechaniczne urządzenia liczące, które potrafią realizować całkiem złożone programy, zazwyczaj nie zalicza się ich do komputerów. Warto jednak pamiętać, że prawzorem komputera jest abstrakcyjny model zwany maszyną Turinga, a pierwsze urządzenia ułatwiające obliczenia były znane w starożytności, np. abakus z 440 p.n.e.

W początkowym okresie rozwoju komputerów budowano komputery zerowej generacji na przekaźnikach i elementach mechanicznych.

Właściwie wszystkie współczesne komputery to maszyny elektroniczne. Próby budowania komputerów optycznych (wykorzystujących przełączniki optyczne), optoelektronicznych (połączenie elementów optycznych i elektronicznych), biologicznych (wykorzystujące wypreparowane komórki nerwowe) czy molekularnych (wykorzystujące jako bramki logiczne pojedyncze cząsteczki) są jeszcze w powijakach i do ich praktycznego zastosowania jest wciąż długa droga. Innym rodzajem komputera jest komputer kwantowy, którego układ przetwarzający dane wykorzystuje efekty fizyczne wynikające z mechaniki kwantowej. 

To, co odróżnia współczesne komputery od wszystkich innych maszyn, to możliwość ich programowania, czyli wprowadzenia do pamięci komputera listy instrukcji, które mogą być wykonane w innym czasie.

W większości przypadków instrukcje, które komputer wykonuje, są bardzo proste – dodawanie dwóch liczb, przeniesienie danych z jednego miejsca w inne, wyświetlenie komunikatu itd. Instrukcje te odczytywane są z pamięci komputera i zazwyczaj wykonywane są w tej samej kolejności, co w pamięci. Istnieją jednak instrukcje umożliwiające „skok” w pewne określone miejsce programu i wykonanie go z tego miejsca. Ponadto instrukcje skoku mogą być wykonane warunkowo, co umożliwia wykonanie różnych zestawów instrukcji w zależności od uzyskanych wcześniej wyników obliczeń. Ponadto istnieją instrukcje umożliwiające tworzenie podprogramów.

Programowanie można w pewnym stopniu przyrównać do czytania książki. W większości wypadków słowa odczytywane są po kolei, zdarzają się jednak momenty, gdy czytelnik wraca do wcześniejszego rozdziału lub omija nieciekawy fragment. Komputery mają możliwość wykonania pewnych instrukcji w pętli, dopóki nie zostanie spełniony jakiś warunek.

Można tu użyć przykładu człowieka próbującego zsumować kolejne liczby na kalkulatorze. Dodaje 1 + 2, do wyniku dodaje 3 itd. Przy próbie zsumowania 10 liczb nie stanowi to problemu, jednak już przy tysiącu po pierwsze zajmuje to bardzo dużo czasu, po drugie przy tak dużej ilości operacji istnieje duże prawdopodobieństwo błędu. Komputer z kolei wykona tę operację w ułamku sekundy, przy użyciu prostego programu

Produkcja komputerów mainframe zaczęła się pod koniec roku 1950. Pierwsze komputery mainframe produkowały firmy nazywane „IBM and the Seven Dwarfs” („IBM i siedmiu krasnoludków”): IBM, Burroughs, Control Data Corporation, General Electric, Honeywell, NCR, RCA, i UNIVAC. Dominacja IBM, osiągnięta dzięki ich systemowi 700/7000, jeszcze wzrosła dzięki przełomowej serii System/360. Z czasem technologia ta rozwijała się i ewoluowała do obecnej „z/Architecture”. UNIVAC został wchłonięty przez Sperry, który kontynuował rozwój tej linii w postaci 2200 Series z systemem operacyjnym OS/2200. Burroughs rozwijał swoją linię B5000 z systemem operacyjnym MCP. Jednocześnie dostawcami spoza USA byli: Siemens, Telefunken w Niemczech, ICL w Anglii oraz Fujitsu, Hitachi, Oki i NEC w Japonii[potrzebny przypis]. Podczas zimnej wojny państwa należące do Układu Warszawskiego produkowały klony „System/360” pod nazwą RIAD. W Polsce produkowano własne konstrukcje oraz klony mainframów ICL pod nazwą Odra. W Związku Radzieckim opracowywano własne systemy, takie jak Ural i Strela.

W latach 70 XX w. i 80 XX w. korporacje odkryły rozwiązania bazujące na minikomputerach, które można było uruchomić za ułamek ceny mainframe i które oferowały lokalnym użytkownikom o wiele większą kontrolę nad systemem. Terminale używane do interakcji systemu mainframe z użytkownikiem zostały zastąpione poprzez komputery osobiste. Nowe instalacje systemów mainframe ograniczyły się do nielicznych zastosowań, takich jak np. kluczowe hurtownie danych. Honeywell zostało wykupione przez Bull, UNIVAC został działem Sperry Corporation, który potem w roku 1986 połączył się z Burroughs w firmę o nazwie Unisys. Sperry i Burroughs po fuzji ciągle wspierają oraz rozwijają swoje mainframe, które obecnie wyewoluowały w platformę Clearpath Plus. W roku 1991 AT&T przemianowano na NCR. 

W przeciwieństwie do komputerów osobistych konsole mają ograniczony interfejs użytkownika i dostęp do ustawień administracyjnych. Zazwyczaj nie jest również możliwe unowocześnianie podzespołów[1]. W niektórych konsolach możliwe jest zainstalowanie w pełni funkcjonalnego systemu operacyjnego. Komputer zwykle jest umieszczony w niedużej obudowie wyposażonej w niezbędne złącza (dla gamepadów, telewizora, zasilania i innych). 

Superkomputer – komputer znacznie przewyższający możliwościami powszechnie używane komputery, w szczególności dysponujący wielokrotnie większą mocą obliczeniową. Określenie pojawiło się w latach 60. XX w. w odniesieniu do komputerów produkowanych przez CDC i później przez przedsiębiorstwo Cray. Były one produkowane w dziesiątkach egzemplarzy i kosztowały po kilka milionów dolarów. Współcześnie większość superkomputerów to pojedyncze egzemplarze, zaprojektowane i wyprodukowane na zamówienie, zazwyczaj z seryjnie produkowanych procesorów i innych podzespołów. Koszty ich produkcji sięgają miliarda dolarów[1]. Od czerwca 2020 roku najszybszym superkomputerem na świecie jest Fugaku, wybudowany w Japonii.

Postęp techniczny w dziedzinie informatyki powoduje, że każdy superkomputer staje się przestarzały w ciągu kilku lat i jego używanie przestaje być opłacalne. Maszyny zaliczane dwadzieścia lat temu do klasy superkomputerów miały wydajność porównywalną z dzisiejszymi urządzeniami przenośnymi. Przykładowo IPhone_XS w teście LINPACK uzyskuje wynik 7,5 GFLOPS, co w 1993 roku stawiałoby go na 30 miejscu wśród najszybszych superkomputerów świata[2][3].

Superkomputery używane są głównie do przeprowadzania złożonych fizycznych symulacji, takich jak prognozowanie pogody, badania zmian klimatu, modelowanie reakcji chemicznych, badanie aerodynamiki samolotów czy badania procesów starzenia broni termojądrowej. 

Główną miarą wydajności obliczeniowej, stosowaną obecnie dla superkomputerów, jest liczba wykonywanych w ciągu sekundy operacji na liczbach zmiennoprzecinkowych w precyzji 64-bitowej(FLOPS). Podaje się ją często z odpowiednim przedrostkiem SI. Przykładowo teraflops („TFLOPS”) to 1012 FLOPS, a petaflops („PFLOPS”) to 1015 FLOPS. Mierzy się ją za pomocą odpowiednich testów wzorcowych. Najpopularniejszym takim testem jest LINPACK, mierzący szybkość rozwiązywania gęstych układów równań liniowych za pomocą metody Gaussa. Ponieważ wiele problemów stawianych superkomputerom daje się sprowadzić do rozwiązywania takich równań, test ten jest wygodną abstrakcją pomiaru efektywności w rozwiązywaniu takich problemów. Superkomputery uzyskujące najwyższe wyniki w tym teście są od 1993 roku publikowane na liście TOP500, aktualizowanej dwa razy w roku.

Inną miarą jest sprawność energetyczna superkomputera wyrażana w jednostkach FLOPS/wat, uwzględniająca zużycie prądu przez superkomputer. Najefektywniejsze według tej miary superkomputery z listy TOP500 są publikowane od 2007 roku na liście Green500. 

Oszustwo "na BLIK-a" to jedna z najpopularniejszych metod stosowanych przez przestępców. Wymaga jedynie umiejętności socjotechnicznych. Niekiedy przestępcy stosujący ten atak są bardzo młodzi. Zatrzymany przez policję mężczyzna ma zaledwie 18 lat.

Działania policjantów z Wałbrzycha doprowadziły do zatrzymania młodego cyberprzestępcy. 18-latek wypłacał gotówkę z bankomatów, wyłudzając na Facebooku kody BLIK. Na tym etapie śledztwa, mężczyźnie udowodniono dokonanie 6 tego typu przestępstw. Trwa ustalanie, ile mogło być ich w rzeczywistości.

Udowodniono mu wyłudzenie łącznie sześciu tysięcy złotych. Przestępca wpadł w ręce policji podczas... wypłacania gotówki z bankomatu. Szybko okazało się, że oszustwo na BLIK to nie jedyna nielegalna forma zarobku prowadzona przez 18-latka. W jego mieszkaniu znaleziono spore ilości narkotyków. Mężczyzna ukrywał ponad 1000 porcji handlowych amfetaminy w chlebie.


Policjanci zabezpieczyli również kilka starszych modeli telefonów komórkowych, które zapewne służyły do kontaktu z klientami. W mieszkaniu znaleziono też 24 kg krajanki tytoniowej bez polskiej akcyzy, co przeliczono na stratę dla Skarbu Państwa na łącznie ponad 24 tysiące złotych. Podejrzany decyzją sądu został tymczasowo aresztowany na okres 3 miesięcy. Sprawa ma charakter rozwojowy.  
Metoda "na BLIK-a" nadal skuteczna

Oszustwo metodą "na BLIK-a" to nadal jedna z najpopularniejszych metod. Jest zasadniczo bardzo prosta w działaniu i niezwykle skuteczna. Najpierw przestępca włamuje się na konto Facebookowe przypadkowej osoby. A to bywa zaskakująco proste, ponieważ wiele osób nadal nie stosuje zabezpieczeń 2FA, a także wykorzystuje kiepskie hasła. W innym przypadku, złodzieje próbują zalogować się do konta Facebookowego z wykorzystaniem loginów i haseł, które trafił do sieci w wyniku wycieku danych z innego serwisu.

Następnie oszust, kiedy już posiada dostęp do czyjegoś konta, zaczyna konwersację ze znajomymi danego użytkownika. Za każdym razem schemat jest podobny. Oszust sugeruje, że właśnie robi zakupy i zabrakło mu pieniędzy, jest w pilnej potrzebie, nie działa mu karta płatnicza, czy tym podobne. Prosi więc o kod BLIK, a następnie korzystając z niego wypłaca pieniądze z bankomatu. 

Uwadze niektórych entuzjastów branży IT umnkął niedawno pewien porażający detal na temat zmian w firmie Amazon: nowy człowiek w radzie nadzorczej. Zmiany w managemencie dużych firm rzadko uchodzą za ekscytujące informacje, ale tym razem osoba, o którą chodzi to generał Keith Alexander. Tak jest: ten Keith Alexander, dyrektor służby NSA w czasach implementacji, uruchomienia oraz zdemaskowania projektu PRISM, o którym świat dowiedział się wskutek zdrady, jakiej dokonał Edward Snowden.

Amazon jest obecnie w dziedzinie usług chmurowych tym, czym dla komputerów w latach siedemdziesiątych był IBM: defniuje on branżę. Olbrzymia liczba usług, z których wiele należy do tzw. usług wysokiego zaufania (medycyna, wywiad, służby), działa w oparciu właśnie o Amazon Web Services. Firma ta jest na dobrej drodze, by stać się wszechbranżowym monopolistą, a nie brakuje opinii, że takie interesy, jak trwająca właśnie współpraca z CIA, zapewnią mu nienaruszalność i ochronę przed prawnym uregulowaniem.
Człowiek o jasnych poglądach

Keith Alexander uchodzi za jastrzębia NSA. Bronił on programu PRISM oraz zestawu pozostałych metod globalnego podsłuchu całego internetu i głośno wyrażał opinię, że publika zbyt mocno wierzy prasie w kwestii legalności podsłuchów. Sugerował, że jest to jedyna droga walki z terroryzmem i że sugerowanie, że "skoro władza może podsłuchiwać, to na pewno to robi" jest bezczelnością. Snowdena uznaje za zagrożenie dla narodowego bezpieczeństwa.

Mamy tu do czynienia z intrygującym zestawieniem. Firma monopolizująca cyfrowe życie i dostarczająca sporą część internetu rozpoczyna współpracę z autorem największego programu podsłuchowego w dziejach ludzkości. Generał Alexander oraz Jeff Bezos to niewątpliwie ta sama półka. Bardzo wartościowym źródłem pozwalającym wyrobić sobie opinię na temat tego pierwszego, jest jego wystąpienie na konferencji Black Hat 2013, gdzie był gościem honorowym. Jest ono wartościowe także z innych powodów merytorycznych.

Czy obawy są słuszne?

Amazon współpracujący z CIA i NSA przywodzi na myśl spiskowo brzmiące określenia typu "military-industrial complex" oraz "deep state", co jest szczególnie interesujące w świetle ostatnich wypowiedzi Donalda Trumpa, który stwierdził, że "ci ludzie w Pentagonie ciągle tylko chcą chodzić na wojny, żeby ich kochane firmy zarabiały na zrzucaniu bomb". Tumult, jaki po nich nastąpił wskazuje na to, jak trudno będzie prowadzić dyskusję na temat roli IT w środowiskach wojskowych (i odwrotnie). Całe szczęście, że Bezos mówi o Amazonie 
Google właśnie zapowiedziało Android 11 Go Edition, wersję najnowszego systemu operacyjnego skierowaną dla posiadaczy mniej potężnych smartfonów.

Osoby posiadające telefony z maksimum 2 GB pamięci RAM mogą być nim szczególnie zainteresowane. Przyjrzyjmy się, jaką wydajność i funkcjonalność obiecała firma Google.

Ma być przede wszystkim szybciej – producent chwali się, że aplikacje będą uruchamiać się o 20 procent szybciej niż na Android 10 Go. Zyskamy do 270 MB dodatkowej pamięci RAM i w praktyce powinniśmy być w stanie płynnie działać na telefonie z trzema-czterema aplikacjami w tle.

Ponadto Android 11 Go  będzie nam dostarczał takich nowości, jak: grupowane powiadomienia z komunikatorów, a także jednorazowe potwierdzenia zgody na wykorzystywanie kamery lub mikrofonu.

Co więcej, Android 11 Go wprowadzi także system nawigacji gestami, który został już wcześniej wprowadzony w Android 11. Jeśli chodzi o dostępność, nie ma jeszcze oficjalnej listy.

Jednak jak podaje portal XDA-developers, w tym momencie na rynku jest 367 urządzeń mobilnych, które posiadają maksymalnie 2 GB RAM-u i mają Android 10 na pokładzie (albo jako aktualizacja albo domyślnie). Aktualizacja z Android 10 Go do Android 11 Go ma trafić już w przyszłym miesiącu.

Mapy Google mają wiele zalet, ale nie brakuje im także istotnych wad. Na jedną z nich zwrócili uwagę rolnicy z Norwegii, który w ciągu jednego tygodnia aż 5 razy "bawili się" w pomoc drogową i z pomocą traktora i quadów ratowali kierowców bezmyślnie korzystających z nawigacji. Bezgranicznie ufając Mapom Google, wjeżdżają oni na stromą i grząską drogę przeznaczoną tylko dla maszyn rolniczych – bo tak każe im GPS.

Na problem zwrócił uwagę serwis autoevolution. Iiris Celine opisła swoje doświadczenie w wątku na stronie pomocy technicznej Map Google i jak wynika z opisu, wiele razy zgłaszała oznaczoną drogę jako błędną – niestety bezskutecznie. Jak twierdzi, kontakt z Google jest beznadziejny i na tym etapie nie do końca wiadomo, co można zrobić.

Wątek rozpoczęto jeszcze w sierpniu, ale do dzisiaj nie pojawiła się jednoznaczana odpowiedź, czy problem został rozwiązany. Wszystko wskazuje jednak na to, że niestety nic się w tej sprawie nie zmieniło – droga wskazana przez rolników jako nieprzejezdna dla samochodów w chwili pisania niniejszej publikacji wciąż widnieje na mapie jako dostępne połączenie między drogą Fv698 a turystycznym Stryn.

Mapy Google nie pierwszy raz winione są w zasadzie niesłusznie – powodem problemów jest bowiem przede wszystkim bezmyślność kierowców i bezgraniczne ufanie wskazaniom nawigacji. W lutym opisywaliśmy przypadek z USA, gdzie mężczyzna – zawierzywszy wskazaniom Map Google – wjechał samochodem do rzeki. Inny ciekawy przypadek to żalenie się użytkownika z Indii na funkcjonalność Map Google, który twierdzi, że aplikacja psuje stosunki w jego rodzinie.

Na moim blogu miałem przyjemność testowania wszelkiej jakości myszek przewodowych, jak i bezprzewodowych, skierowanych do prostych zadań biurowych, a także tych z segmentu rozrywkowego. Mimo tego że właśnie ten ostatni rodzaj pojawia się na moim blogu nader często, najwięcej uwagi poświęcam urządzeniom biurowym. Z jednej strony najwięcej czasu spędzam przy pracowniczym komputerze, do którego wymagam wysokiej jakości urządzeń wskazujących a z drugiej - moje nadgarstki mówią jasno - oznaki przemęczenia i wstępne oznaki zwyrodnienia nadchodzą nieubłaganie prędko.

Jak zatem obronić się przed nieuniknionym? Można robić przerwy, wykonywać ćwiczenia lub po prostu zmienić sprzęt / dostosować go pod długą pracę przy komputerze. W moim wypadku jest różnie, choć i tak ostatecznie ląduję przed sprzętem do montażu gdzie ślęczę długie godziny przed projektami. Postanowiłem więc działać i przestawiłem się właśnie na myszki wertykalne, które dzięki swojej nienaturalnej budowie, wprowadzają naturalny chwyt. Dłoń ułożona w ten sposób wykonuje o wiele mniej ruchów i minimalizuje niechciane efekty uboczne. 
Mistrz ergonomii

O możliwościach technicznych i pełnym zrecenzowaniu tego modelu, dogłębnie rozpisali się już użytkownicy portalu - Pangrys i wojtekadams, więc postanowiłem nie dublować ich pracy, skupiając się na własnych przemyśleniach dotyczących dłuższej pracy na nim. Urządzenie testuję już ponad miesiąc, przesiadając się od czasu do czasu na budżetowy model Natec'a Euphonie, który nie tak dawno opisywałem na łamach bloga. Mimo iż jest to model stosunkowo tani, oferuje wystarczające możliwości przy świetnie przystosowanej pod dłoń osoby dorosłej, obudowie. Podobnie jest zresztą z modelem Natec Crake, będącym wczesną wersją Euphonie i posiadającą - wg. mnie - jeszcze wygodniejszą obudowę. Mimo wszystko nagłe przejście z modeli budżetowych na klasę premium jest bardziej niż zadowalające.

Przemawia za tym przede wszystkim zastosowanie wysokiej klasy elementów konstrukcyjnych oraz krzywa budowy, mieszcząca się w magicznych 57 stopniach. W związku z tym iż jest to sprzęt ze stajni Logitech, otrzymujemy również dedykowane wsparcie oprogramowania Logitech Options, uprawniające nas do sprawnego zarządzania skrótami, makrami oraz opcjami konfiguracyjnymi tego modelu. Sprawne przejście między urządzeniami nie sprawiło mi wiele trudności, choć przyzwyczajenie się do znacznie lepszej gramatury powierzchni wymagało u mnie nieco więcej czasu. Największym "za" przy modelu MX Vertical należy uznać jego możliwości dodatkowe, które konkurencję pozostawiają daleko w tyle.


Należy jednak sprostować na początku jedną rzecz. Myszka wertykalna Logitech MX Vertical nie będzie urządzeniem dla każdego. Jest to model skierowany głównie dla osoby dorosłej, choćby przez zastosowanie dużej powierzchni na chwyt dłoni. Przy rączkach osoby młodszej, możemy napotkać pewne problemy podczas sterowania czy odpowiedniego ułożenia dłoni na przyciskach. To samo tyczy się także wygody przesuwania rolki czy używania skrótów zlokalizowanych pod kciukiem. 

Jeśli grupą docelową jest osoba dorosła to w mig pojmie jak wielkim dobrem jest właśnie taka lekko przekrzywiona myszka biurowa. 
Przygotowanie do pracy

W moim przypadku myszka oprócz posiadania odpowiednich parametrów pozwalających na wygodną, ale i zdrową dla nadgarstków pracę, musiała posiadać przynajmniej dwa skróty fizyczne pod konfigurację oraz prosty przycisk do zmiany DPI w locie. Wszystko przez to, że prócz pracy montażysty na stanowisku promocyjnym, zajmowałem się także różnorakimi zadaniami biurowymi. Praca na dwóch, a nawet trzech monitorach, wymagała od urządzenia wysokiej czułości a w przypadku podmiany ścieżek, ustawiania wartości czy zabawy kolorystycznej, niskich wartości DPI i ich szybkiej zmiany. Szybko okazało się, że MX Vertical radzi sobie z tym bez żadnych komplikacji. 
image
image
image
image

Po przyzwyczajeniu dłoni pod nowe ustawienia i odpowiedniej konfiguracji w dedykowanym oprogramowaniu producenta mogłem przystąpić do pracy właściwej. Osoby na co dzień korzystające z takich programów jak GIMP, Adobe Premiere Pro czy DaVinci Resolve, dobrze wiedzą jak wymagana jest dobrej jakości myszka do zaawansowanej pracy biurowej. W dedykowanym oprogramowaniu możemy więc ustawić pod siebie przyciski funkcyjne myszki, a także zwiększyć lub zmniejszyć precyzję szybkości wskaźnika. 
image
image

Początkowo nie mogłem zaakceptować tego, że myszka ta nie posiada kółka grawitacyjnego do łatwiejszego przeglądania wielu stron naraz, ale z drugiej strony dla mnie najbardziej liczyła się precyzja, więc po pewnym czasie się do tego przyzwyczaiłem. 
Kultura pracy

Logitech MX Vertical kupił mnie nie tylko naturalną pozycją ułożenia dłoni na urządzeniu, ale także 57-stopniowym nachyleniem, które w tym wypadku jeszcze lepiej spisuje się podczas pracy biurowej. Ten "prawie" pionowy chwyt jest trudny do opanowania ale gdy będziemy mieli już to za sobą, to docenimy wartości, jakie za sobą niesie. Po trzech tygodniach pracy zmniejszyła mi się ilość przemęczeń nadgarstka, a także ilość niepotrzebnych ruchów generowanych przez myszki płaskie, została pomniejszona o ponad 40%. To świetny wynik z poziomu ergonomii, widząc ile niepotrzebnych ruchów wykonujemy za dnia. 
image
image

Chropowata powierzchnia również świetnie spełnia swoją funkcję, gdyż przez cały okres testów wydawało mi się, że zmniejsza ona w ten sposób potliwość dłoni. Wyżłobienia w lepszy sposób wentylują wewnętrzną część dłoni i uwaga - prawie w ogóle nie zbierają tłuszczu czy zabrudzeń. Przetarcie raz na jakiś czas urządzenia szmatką będzie obowiązkowe, ale nie tak często, jak myszki pokryte tworzywem sztucznym. 
image
image

Także i w tym modelu postawiono na funkcję łączności Bluetooth oraz poprzez nadajnik unifying, ale nic nie stoi na przeszkodzie, by wykorzystać sterowanie po kablu. Co prawda w tym wypadku mamy do czynienia z myszką działającą głównie bezprzewodowo, ale jest także możliwość jednoczesnego ładowania, jak i działania urządzenia poprzez uwiązanie z komputerem. Jednak nie polecam takiego środka działania, gdyż jest ono niewygodne. Dużym plusem jest jednak fakt, iż godzinne podładowanie akumulatora, zwiększy żywotność myszki do 4 miesięcy ciągłego działania.
image
image

Producent zastosował w tym modelu także możliwość przełączania się w locie między trzema urządzeniami, technologią Easy-siwtch i Flow, znaną m.in z poprzednich modeli.
Towarzysz twórcy

Urządzenie otrzymałem na testy długofalowe, więc pozostanie ono u mnie na nieco dłużej. Przez ten czas, w ramach dodatkowych testów, chciałbym sprawdzić jak poradzi sobie akumulator w różnych kulturach, a także miejscach pracy. Będę starał się wykrzesać z myszki najwyższą wydajność, nie szczędząc jej wewnętrznie, jak i zewnętrznie. Na tę chwilę - jak zresztą można zauważyć na zdjęciach podglądowych - urządzenie nieźle trzyma się jak na sześć tygodni testów w warunkach biurowych. Muszę przyznać, iż urządzenie będzie ciężko zajechać w nieco wymagających aspektach pracy przy komputerze, co dobrze znaczy o zastosowanych komponentach. Czując niedosyt testerski, rozpisałem niecny plan działania w trudnych warunkach wyjazdowych, pracy zdalnej, a także użytkowania myszki w obiektach o podwyższonej temperaturze / wilgotności powietrza, by sprawdzić jak poradzi sobie w testach długofalowych.
image
image

Jeśli jednak ktoś z czytelników, byłby zainteresowany tym modelem, to powinien się pospieszyć, gdyż niedawno jego cena stała się nader atrakcyjna. Obecnie za Logitech MX Vertical przyjdzie nam zapłacić około co będzie niezłym kąskiem dla osób poszukujących wysokiej jakościowo myszki wertykalnej do pracy biurowej. Jak macie pytania dotyczące działania lub funkcjonalności, piszcie. Postaram się na nie odpowiedzieć 






